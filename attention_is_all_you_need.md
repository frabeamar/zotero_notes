Self-attention existed before the transformer

RNN are autoregressive model, consume the input to generate the next token

Batch\_norm : mean and std over that batch

Layer-norm: mean,variance over the features

ADDITIVE ATTENTION:

ei​=vTtanh(Wq​q+Wk​ki​)